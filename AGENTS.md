# AGENTS.md

This document provides an overview of the architecture, components, models, workflows, and instructions that enable AI agents to collaborate effectively within the monorepo project.

---

## Project Overview

The monorepo currently contains two main applications:

1. **frontend/** (Next.js v15, App Router, TailwindCSS, TypeScript)

   * Web interface for users to interact with AI features
   * Features include: voice-enabled chatbot, image-to-video generation, avatar-based UI

2. **itv-api/** (FastAPI Python backend)

   * API service responsible for handling avatar generation tasks
   * Connects with image-to-video models (AnimateDiff or placeholders)

All subprojects are structured to support eventual deployment and integration with AI agents such as OpenAI Codex.

---

## Monorepo Architecture: iaai

### Root

```bash
.
â”œâ”€â”€ AGENTS.md                 # Agent-focused system documentation
â”œâ”€â”€ Makefile                  # Top-level helper for common commands
â”œâ”€â”€ package.json              # Shared scripts or tooling (optional)
â”œâ”€â”€ tsconfig.json             # Shared TypeScript config for frontend/backend TS code
â”œâ”€â”€ README.md                 # Root-level overview
â”œâ”€â”€ apps/                     # Application-specific code
â”œâ”€â”€ packages/                 # Shared or standalone libraries (LLM, RAG, TTS, etc.)
```

### apps/

```bash
apps/
â”œâ”€â”€ frontend/                 # Next.js 15 UI (main interface)
â”‚   â”œâ”€â”€ public/               # Static assets (icons, action images, etc.)
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app/              # App router pages/api endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ video-generator/page.tsx
â”‚   â”‚   â”‚   â””â”€â”€ api/          # Internal proxies to Python APIs (e.g., chat, speak, image-to-video)
â”‚   â”‚   â”œâ”€â”€ components/       # UI components
â”‚   â”‚   â”œâ”€â”€ hooks/            # React hooks (e.g., useChat, useTextToSpeech)
â”‚   â”‚   â”œâ”€â”€ layout/           # Layout wrappers
â”‚   â”‚   â”œâ”€â”€ mock/             # Static mock data
â”‚   â”‚   â”œâ”€â”€ services/         # API service wrappers (e.g., generateVideo)
â”‚   â”‚   â”œâ”€â”€ styles/           # CSS/Tailwind
â”‚   â”‚   â”œâ”€â”€ types/            # TypeScript interfaces (e.g., ChatMessage, StreamChunk)
â”‚   â”‚   â””â”€â”€ utils/            # Utilities (e.g., batching, highlight parsing)
â”‚   â””â”€â”€ tsconfig.json        # Frontend-specific TypeScript config
â”œâ”€â”€ api/
â”‚   â””â”€â”€ image-to-video/       # FastAPI service for AnimateDiff
â”‚       â”œâ”€â”€ app/
â”‚       â”‚   â”œâ”€â”€ api.py        # API endpoints
â”‚       â”‚   â”œâ”€â”€ core.py       # Core business logic
â”‚       â”‚   â””â”€â”€ main.py       # FastAPI app instance
â”‚       â”œâ”€â”€ models/
â”‚       â”‚   â””â”€â”€ model_stub.py # Placeholder or inference module
â”‚       â”œâ”€â”€ assets/           # Temp storage for test/demo files (excluded in .gitignore)
â”‚       â”œâ”€â”€ requirements.txt  # Python dependencies
â”‚       â”œâ”€â”€ Makefile          # Developer workflow
â”‚       â””â”€â”€ README.md         # API-specific usage guide
```

### packages/

```bash
packages/
â”œâ”€â”€ shared/                   # Shared TypeScript utilities (interfaces, types, helper functions)
â”œâ”€â”€ llm/                      # LLM tooling (OpenAI, Ollama, prompt templates)
â”œâ”€â”€ rag/                      # Retrieval-augmented generation logic (e.g., vector search)
â””â”€â”€ tts/                      # Text-to-speech modules (e.g., OpenVoice interface)
```

### Other Adjustments

.DS_Store, **pycache**, and assets/ should be included in the top-level .gitignore.

All README.md files in apps/ and packages/ should be local to their respective context.

Future agents should reference paths as apps/frontend/..., packages/shared/..., etc.

---

## Integrated AI Models

### AnimateDiff (Planned via ComfyUI)

* **Purpose**: Converts a still image and motion prompt into a short video
* **Integration Plan**:

  * Model will run on a locally-hosted ComfyUI instance
  * API endpoint `/generate` will POST an image and action string
  * ComfyUI will generate a video from the image
* **Expected Input**:

  ```json
  {
    "image": <image file>,
    "action": "kung_fu"
  }
  ```

* **Expected Output**:

  * MP4 video file path served back to frontend

### ðŸŽ¤ OpenVoice (Voice Cloning, Planned)

* **Purpose**: Clone a user's voice and generate speech in that style
* **Features**:

  * Multi-style control (sad, sarcastic, happy, etc.)
  * Offline voice synthesis (no API calls)
* **Integration Plan**:

  * Train a voice profile from user sample
  * Use sentiment analysis to control tone
  * Replace ElevenLabs for TTS

---

## Prompt Patterns & Agent Instructions

When operating Codex or any AI agents over this monorepo:

* Prefer **Next.js App Router** features in the `frontend/`
* Use **TailwindCSS** for styling
* All TTS or voice playback logic lives in `useTextToSpeech.ts`
* Use `SmartTTSBatcher` for streaming sentence-aware voice output
* To add new features to the image-to-video system:

  1. Add a new action in `ActionSelector.tsx`
  2. Connect it to a supported motion template in the backend

---

## Known Constraints & TODOs

* AnimateDiff integration is currently mocked via a video stub (see `model_stub.py`)
* The `.venv` environment and video assets are gitignored
* Future versions will:

  * Integrate OpenVoice as a local TTS system
  * Replace video stubs with ComfyUI-generated outputs

---

## Getting Started with the Monorepo

To run everything locally:

```bash
# Setup backend
cd itv-api
make init  # creates venv
make install  # installs deps
make dev  # launches FastAPI with Uvicorn

# Setup frontend
cd ../frontend
pnpm install
pnpm dev
```

---

For more detailed usage and component guidance, see the `README.md` files inside each subproject.

This AGENTS.md will be expanded as the system evolves.
