# IAAI Monorepo

A monorepo for the **Interactive AI Avatar Interface (IAAI)**, designed to integrate a naturalistic AI avatar that supports:

* ğŸ™ï¸ Voice-to-voice communication with sentiment-driven tone modulation
* ğŸ§  Locally hosted LLM responses via Ollama
* ğŸ—£ï¸ TTS via OpenVoice or other local models
* ğŸ¬ Image-to-video generation using AnimateDiff (mocked via ComfyUI `model_stub.py`)
* ğŸ¤– Agent support via OpenAI Codex using an `AGENTS.md` file

---

## ğŸ—‚ Project Structure

```bash
.
â”œâ”€â”€ AGENTS.md
â”œâ”€â”€ Makefile
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â”œâ”€â”€ apps
â”‚   â”œâ”€â”€ api
â”‚   â”‚   â””â”€â”€ image-to-video
â”‚   â”‚       â”œâ”€â”€ app
â”‚   â”‚       â”‚   â”œâ”€â”€ api.py
â”‚   â”‚       â”‚   â”œâ”€â”€ core.py
â”‚   â”‚       â”‚   â””â”€â”€ main.py
â”‚   â”‚       â”œâ”€â”€ assets/              # Local video/image assets (ignored by Git)
â”‚   â”‚       â”œâ”€â”€ models
â”‚   â”‚       â”‚   â””â”€â”€ model_stub.py    # AnimateDiff mock model
â”‚   â”‚       â”œâ”€â”€ Makefile
â”‚   â”‚       â”œâ”€â”€ requirements.txt
â”‚   â”‚       â””â”€â”€ README.md
â”‚   â””â”€â”€ frontend
â”‚       â”œâ”€â”€ public/
â”‚       â”œâ”€â”€ src/
â”‚       â”‚   â”œâ”€â”€ app/                 # Next.js app router
â”‚       â”‚   â”œâ”€â”€ components/
â”‚       â”‚   â”œâ”€â”€ hooks/
â”‚       â”‚   â”œâ”€â”€ layout/
â”‚       â”‚   â”œâ”€â”€ services/
â”‚       â”‚   â”œâ”€â”€ theme/
â”‚       â”‚   â”œâ”€â”€ types/
â”‚       â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ README.md
â”‚       â””â”€â”€ package.json
â”œâ”€â”€ packages
â”‚   â”œâ”€â”€ llm/                         # Shared LLM logic (OpenAI/Ollama abstraction)
â”‚   â”œâ”€â”€ rag/                         # Retrieval-augmented generation (optional)
â”‚   â”œâ”€â”€ shared/                      # Shared types, constants, helpers
â”‚   â””â”€â”€ tts/                         # Local TTS model wrappers (e.g., OpenVoice)
```

---

## ğŸš€ Getting Started

### 1. Clone the Monorepo

```bash
git clone git@github.com:your-user/iaai-monorepo.git
cd iaai-monorepo
```

### 2. Install Dependencies

```bash
npm install # or use pnpm, bun, yarn
```

### 3. Start Frontend & API

```bash
# Run frontend
cd apps/frontend
npm run dev

# Run API backend
cd ../api/image-to-video
make dev  # uses .venv and uvicorn
```

---

## ğŸ¤– Agent Integration

Codex agents can interrogate this repo using `AGENTS.md`, which includes detailed metadata on:

* Tools and endpoints
* Prompt templates
* API contracts
* Project context

---

## ğŸ“¦ Package Guidance

* **tts/**: OpenVoice, RVC, or Bark wrappers and utils
* **llm/**: Streaming Ollama interface, OpenAI abstraction
* **rag/**: LangChain chains, context stores (Chroma, FAISS)
* **shared/**: TS types, text utilities, parsers, constants
* **apps/api/image-to-video/**: AnimateDiff-based image-to-video API (currently mocked via `model_stub.py`), planned ComfyUI support
* Future AI model integrations and shared logic will be added under `packages/`

---

## âœ… TODOs

* [x] Frontend: Image upload, action selector, video preview
* [x] Backend: `/generate` endpoint with stub logic for AnimateDiff
* [ ] Integrate full AnimateDiff inference pipeline (currently mocked)
* [ ] Add ComfyUI integration for image-to-video generation
* [ ] Add audio pipeline (TTS + STT + prosody control)
* [ ] Voice cloning and emotion tuning via OpenVoice
* [ ] Complete AGENTS.md coverage for Codex agents

---

## ğŸ“„ License

MIT or similar OSS license (to be decided).
