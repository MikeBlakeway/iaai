# Interactive AI Avatar Interface (IAAI)

An interactive UI prototype for a web-based avatar application that connects to a Large Language Model (LLM), synthesizes speech, and renders responses through a lifelike animated avatar. This repository implements a frontend-first approach using **Next.js**, **TailwindCSS**, and **TypeScript**.

---

## ğŸš€ Features

- âœ¨ Clean, responsive UI scaffold using TailwindCSS
- ğŸ§  Simulated avatar chat interface (mocked backend)
- ğŸ—¨ï¸ Live-rendered chat messages with user/LLM threading
- ğŸ”Š Placeholder for future speech synthesis integration
- ğŸ§‘â€ğŸ¤ Avatar display section for Ready Player Me integration

---

## ğŸ§± Tech Stack

- **Framework**: [Next.js 15 (App Router)](https://nextjs.org)
- **Styling**: [TailwindCSS](https://tailwindcss.com)
- **Language**: [TypeScript](https://www.typescriptlang.org)
- **UI**: Responsive layout with modular components

---

## ğŸ“ Folder Structure

```bash
src/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ page.tsx               // Main app page
â”‚   â””â”€â”€ api/
â”‚       â”œâ”€â”€ chat/              // POST /api/chat â€” ChatGPT response mock
â”‚       â””â”€â”€ speak/             // POST /api/speak â€” TTS audio mock
â”œâ”€â”€ components/                // UI components
â”‚   â”œâ”€â”€ Header.tsx
â”‚   â”œâ”€â”€ Avatar.tsx
â”‚   â”œâ”€â”€ ChatArea.tsx
â”‚   â””â”€â”€ InputBar.tsx
â”œâ”€â”€ layout/
â”‚   â””â”€â”€ Layout.tsx
â”œâ”€â”€ mock/
â”‚   â””â”€â”€ chatMock.ts
â””â”€â”€ styles/
â””â”€â”€ globals.css
```

---

## ğŸ› ï¸ Getting Started

### Prerequisites

- Node.js `>= 18`
- Yarn (preferred)

### Installation

```bash
# Clone the repo
git clone https://github.com/your-username/iaai-ui.git
cd iaai-ui

# Install dependencies
yarn install

# Start the dev server
yarn dev
```

Then open [http://localhost:3000](http://localhost:3000) in your browser.

---

## ğŸ“¤ API Routes

| Endpoint          | Description                         |
| ----------------- | ----------------------------------- |
| `POST /api/chat`  | Simulates ChatGPT-style response    |
| `POST /api/speak` | Simulates speech synthesis response |

> These endpoints return mocked data and are ready to be replaced with real API calls to OpenAI and Eleven Labs.

---

## ğŸ“Œ Next Steps

- âœ… Finalize mock-based rendering workflow
- ğŸ”„ Replace `/api/chat` with OpenAI API integration
- ğŸ”ˆ Connect `/api/speak` to Eleven Labs / Google TTS
- ğŸ§‘â€ğŸ¨ Embed avatar via Ready Player Me with lip sync
- ğŸ§  Implement RAG-based context injection via LangChain

---

## ğŸ” Environment Variables

When integrating real APIs, create a `.env.local` file:

```env
OPENAI_API_KEY=your-key-here
ELEVENLABS_API_KEY=your-key-here
```

Use `process.env` in your server-side handlers to securely access secrets.

---

## ğŸ‘¨â€ğŸ’» Author

Created by [@Mike](https://github.com/MikeBlakeway) â€” built for rapid prototyping and LLM experimentation.

---

## ğŸ“„ License

MIT License. Use freely with attribution.
